{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc34624-40c6-43e1-95c8-455537036b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset, random_split\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from typing import List, Tuple\n",
    "from diffusers import DDPMScheduler, DDIMScheduler, DPMSolverMultistepScheduler\n",
    "import numpy as np\n",
    "\n",
    "scale_bp = 1\n",
    "scale_dfa = 3\n",
    "input_ch = 1\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    "epochs = 31 # for DDPM 31 epochs\n",
    "# epochs = 51 # for DDIM 51 epochs\n",
    "eval_freq = 1\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "time_embed_dim = 100\n",
    "\n",
    "\n",
    "print(\"--- 1. setting ---\")\n",
    "\n",
    "num_timesteps = None\n",
    "while num_timesteps is None:\n",
    "    try:\n",
    "        train_num_timesteps_input_str = input(f\"Select train timestep (e.g. 1000, 250) [default: 250]: \").strip()\n",
    "        if not train_num_timesteps_input_str:\n",
    "            num_timesteps = 250\n",
    "        else:\n",
    "            train_num_timesteps_input = int(train_num_timesteps_input_str)\n",
    "            if train_num_timesteps_input > 0:\n",
    "                num_timesteps = train_num_timesteps_input\n",
    "            else:\n",
    "                print(\"Invalid input. Please enter a positive number.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a number.\")\n",
    "print(f\"Train num_timesteps selected: {num_timesteps}\")\n",
    "\n",
    "SAMPLING_STEPS = {\n",
    "    'ddpm': num_timesteps,\n",
    "    'ddim': 40,\n",
    "    'dpm_solver_2': 40,\n",
    "    'dpm_solver_pp': 40\n",
    "}\n",
    "print(f\"Sampling steps set to: {SAMPLING_STEPS}\")\n",
    "\n",
    "selected_dataset = 'mnist'\n",
    "print(f\"Dataset selected: {selected_dataset}\")\n",
    "\n",
    "class_filter = None\n",
    "try:\n",
    "    class_filter_input = input(\"Enter MNIST digit to use (0-9), or leave blank for all: \").strip()\n",
    "    if class_filter_input:\n",
    "        class_filter = int(class_filter_input)\n",
    "        if not 0 <= class_filter <= 9:\n",
    "            print(\"Invalid digit. Using all digits.\")\n",
    "            class_filter = None\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Using all digits.\")\n",
    "print(f\"Class filter set to: {class_filter}\")\n",
    "\n",
    "EVAL_SAMPLER_TYPE = None\n",
    "while EVAL_SAMPLER_TYPE is None:\n",
    "    sampler_input = input(f\"Select sampler for visualization {list(SAMPLING_STEPS.keys())}: \").strip().lower()\n",
    "    if sampler_input in SAMPLING_STEPS:\n",
    "        EVAL_SAMPLER_TYPE = sampler_input\n",
    "    else:\n",
    "        print(f\"Invalid input. Please choose from {list(SAMPLING_STEPS.keys())}.\")\n",
    "EVAL_SAMPLING_STEPS = SAMPLING_STEPS[EVAL_SAMPLER_TYPE]\n",
    "print(f\"Using [{EVAL_SAMPLER_TYPE}] sampler with {EVAL_SAMPLING_STEPS} steps for visualization.\")\n",
    "\n",
    "USE_DFA = None\n",
    "while USE_DFA is None:\n",
    "    mode_input = input(\"Select training mode (DFA/BP): \").strip().lower()\n",
    "    if mode_input == 'dfa': USE_DFA = True\n",
    "    elif mode_input == 'bp': USE_DFA = False\n",
    "    else: print(\"Invalid input. Please type 'DFA' or 'BP'.\")\n",
    "    \n",
    "if USE_DFA:\n",
    "    scale = scale_dfa # 3\n",
    "    batch_size = 8\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-4\n",
    "else: # BP\n",
    "    scale = scale_bp # 1\n",
    "    batch_size = 128 \n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-4\n",
    "print(f\"Mode: {'DFA' if USE_DFA else 'BP'}, Batch size: {batch_size}, scale: {scale}, LR: {lr}, WD: {weight_decay}\")\n",
    "\n",
    "model_version = None\n",
    "while model_version is None:\n",
    "    model_input = input(\"Select UNet version (1/2): \").strip()\n",
    "    if model_input == '1': model_version = 1\n",
    "    elif model_input == '2': model_version = 2\n",
    "    else: print(\"Invalid input. Please type '1' or '2'.\")\n",
    "print(f\"UNet version selected: UNet{model_version}\")\n",
    "\n",
    "image_size = 28\n",
    "print(f\"Image size selected: {image_size}x{image_size} (Fixed for MNIST)\")\n",
    "print(\"--- 1. setting completed ---\")\n",
    "\n",
    "\n",
    "class Inject_e(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, e, B):\n",
    "        ctx.save_for_backward(e, B)\n",
    "        ctx.grad_output_shape = x.shape\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        e, B = ctx.saved_tensors\n",
    "        target_grad_shape = ctx.grad_output_shape\n",
    "        e_flat = e.view(e.shape[0], -1)\n",
    "        grad_output_est = e_flat.mm(B)\n",
    "        grad_output_est = grad_output_est.view(target_grad_shape)\n",
    "        return grad_output_est, None, None\n",
    "\n",
    "def show_images(images, num_rows=3, num_cols=4, title=\"Generated Images\"):\n",
    "    if not images:\n",
    "        print(\"No images to display.\")\n",
    "        return\n",
    "    fig = plt.figure(figsize=(num_cols * 2, num_rows * 2))\n",
    "    plt.suptitle(title)\n",
    "    for i, img_pil in enumerate(images):\n",
    "        if i >= num_rows * num_cols:\n",
    "            break\n",
    "        ax = fig.add_subplot(num_rows, num_cols, i + 1)\n",
    "        if img_pil.mode == 'L':\n",
    "            ax.imshow(img_pil, cmap='gray')\n",
    "        else:\n",
    "            ax.imshow(img_pil)\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "def _pos_encoding(time_idx, output_dim, device='cpu'):\n",
    "    t, D = time_idx, output_dim\n",
    "    v = torch.zeros(D, device=device)\n",
    "    i = torch.arange(0, D, device=device)\n",
    "    div_term = torch.exp(i / D * math.log(10000))\n",
    "    v[0::2] = torch.sin(t / div_term[0::2])\n",
    "    v[1::2] = torch.cos(t / div_term[1::2])\n",
    "    return v\n",
    "\n",
    "def pos_encoding(timesteps, output_dim, device='cpu'):\n",
    "    batch_size = len(timesteps)\n",
    "    v = torch.zeros(batch_size, output_dim, device=device)\n",
    "    for i in range(batch_size):\n",
    "        v[i] = _pos_encoding(timesteps[i], output_dim, device)\n",
    "    return v\n",
    "\n",
    "\n",
    "class ConvBlock1(nn.Module):\n",
    "    def __init__(self, input_ch, output_ch, time_embed_dim, use_dfa=True):\n",
    "        super().__init__()\n",
    "        activation = nn.Tanh() if use_dfa else nn.ReLU()\n",
    "        self.convs = nn.Sequential(nn.Conv2d(input_ch, output_ch, 5, padding=2), nn.BatchNorm2d(output_ch), activation)\n",
    "        self.mlp = nn.Sequential(nn.Linear(time_embed_dim, input_ch), activation, nn.Linear(input_ch, input_ch))\n",
    "    def forward(self, x, v):\n",
    "        N, C, _, _ = x.shape\n",
    "        v = self.mlp(v).view(N, C, 1, 1)\n",
    "        return self.convs(x + v)\n",
    "\n",
    "class UNet1(nn.Module):\n",
    "    def __init__(self, input_ch=input_ch, time_embed_dim=time_embed_dim, image_size=image_size, batch_size=batch_size, device='cpu', use_dfa=True):\n",
    "        super().__init__()\n",
    "        self.time_embed_dim = time_embed_dim\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.use_dfa = use_dfa\n",
    "        self.down1 = ConvBlock1(input_ch, 64 * scale, time_embed_dim)\n",
    "        self.down2 = ConvBlock1(64 * scale, 128 * scale, time_embed_dim)\n",
    "        self.bot1 = ConvBlock1(128 * scale, 256 * scale, time_embed_dim)\n",
    "        self.up2 = ConvBlock1(128 * scale + 256 * scale, 128 * scale, time_embed_dim)\n",
    "        self.up1 = ConvBlock1(128 * scale + 64 * scale, 64, time_embed_dim)\n",
    "        self.out = nn.Conv2d(64, input_ch, 1)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        if self.use_dfa:\n",
    "            self.e_channels = input_ch\n",
    "            self.e_height = image_size\n",
    "            self.e_width = image_size\n",
    "            self.e = torch.zeros(self.batch_size, self.e_channels, self.e_height, self.e_width, device=self.device)\n",
    "            e_flat_dim = self.e_channels * self.e_height * self.e_width\n",
    "            x1_flat_dim = (64 * scale) * self.image_size * self.image_size\n",
    "            self.Bc1 = nn.Parameter(torch.randn(e_flat_dim, x1_flat_dim), requires_grad=False)\n",
    "            x2_flat_dim = (128 * scale) * (self.image_size // 2) * (self.image_size // 2)\n",
    "            self.Bc2 = nn.Parameter(torch.randn(e_flat_dim, x2_flat_dim), requires_grad=False)\n",
    "            x3_flat_dim = (256 * scale) * (self.image_size // 4) * (self.image_size // 4)\n",
    "            self.Bc3 = nn.Parameter(torch.randn(e_flat_dim, x3_flat_dim), requires_grad=False)\n",
    "            x4_flat_dim = (128 * scale) * (self.image_size // 2) * (self.image_size // 2)\n",
    "            self.Bc4 = nn.Parameter(torch.randn(e_flat_dim, x4_flat_dim), requires_grad=False)\n",
    "    \n",
    "    def forward(self, x, timesteps, noise=None):\n",
    "        v = pos_encoding(timesteps, self.time_embed_dim, x.device)\n",
    "        x1_pre_inject = self.down1(x, v)\n",
    "        x1 = Inject_e.apply(x1_pre_inject, self.e, self.Bc1) if self.use_dfa else x1_pre_inject\n",
    "        x_down1_pooled = self.maxpool(x1)\n",
    "        x2_pre_inject = self.down2(x_down1_pooled, v)\n",
    "        x2 = Inject_e.apply(x2_pre_inject, self.e, self.Bc2) if self.use_dfa else x2_pre_inject\n",
    "        x_down2_pooled = self.maxpool(x2)\n",
    "        x_bot_pre_inject = self.bot1(x_down2_pooled, v)\n",
    "        x_bot = Inject_e.apply(x_bot_pre_inject, self.e, self.Bc3) if self.use_dfa else x_bot_pre_inject\n",
    "        x_up2_upsampled = self.upsample(x_bot)\n",
    "        x_up2_concat = torch.cat([x_up2_upsampled, x2], dim=1)\n",
    "        x_up2_pre_inject = self.up2(x_up2_concat, v)\n",
    "        x_up2 = Inject_e.apply(x_up2_pre_inject, self.e, self.Bc4) if self.use_dfa else x_up2_pre_inject\n",
    "        x_up1_upsampled = self.upsample(x_up2)\n",
    "        x_up1_concat = torch.cat([x_up1_upsampled, x1], dim=1)\n",
    "        x_final_conv = self.up1(x_up1_concat, v)\n",
    "        output_noise_pred = self.out(x_final_conv)\n",
    "        if self.use_dfa and noise is not None:\n",
    "            self.e.data.copy_(output_noise_pred - noise)\n",
    "        return output_noise_pred\n",
    "\n",
    "class ConvBlock2(nn.Module):\n",
    "    def __init__(self, input_ch, output_ch, time_embed_dim, use_dfa=True):\n",
    "        super().__init__()\n",
    "        activation = nn.Tanh() if use_dfa else nn.ReLU()\n",
    "        self.convs = nn.Sequential(nn.Conv2d(input_ch, output_ch, 3, padding=1), nn.BatchNorm2d(output_ch), activation)\n",
    "        self.mlp = nn.Sequential(nn.Linear(time_embed_dim, input_ch), activation, nn.Linear(input_ch, input_ch))\n",
    "    def forward(self, x, v):\n",
    "        N, C, _, _ = x.shape\n",
    "        v = self.mlp(v).view(N, C, 1, 1)\n",
    "        return self.convs(x + v)\n",
    "\n",
    "class UNet2(nn.Module):\n",
    "    def __init__(self, input_ch=input_ch, time_embed_dim=time_embed_dim, image_size=image_size, batch_size=batch_size, device='cpu', use_dfa=True):\n",
    "        super().__init__()\n",
    "        self.time_embed_dim = time_embed_dim\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.use_dfa = use_dfa\n",
    "        self.down1 = ConvBlock2(input_ch, 64 * scale, time_embed_dim)\n",
    "        self.down1_2 = ConvBlock2(64 * scale, 64 * scale, time_embed_dim)\n",
    "        self.down2 = ConvBlock2(64 * scale, 128 * scale, time_embed_dim)\n",
    "        self.down2_2 = ConvBlock2(128 * scale, 128 * scale, time_embed_dim)\n",
    "        self.bot1 = ConvBlock2(128 * scale, 256 * scale, time_embed_dim)\n",
    "        self.bot1_2 = ConvBlock2(256 * scale, 256 * scale, time_embed_dim)\n",
    "        self.up2 = ConvBlock2(128 * scale + 256 * scale, 128 * scale, time_embed_dim)\n",
    "        self.up2_2 = ConvBlock2(128 * scale, 128 * scale, time_embed_dim)\n",
    "        self.up1 = ConvBlock2(128 * scale + 64 * scale, 64, time_embed_dim)\n",
    "        self.up1_2 = ConvBlock2(64, 64, time_embed_dim)\n",
    "        self.out = nn.Conv2d(64, input_ch, 1)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        if self.use_dfa:\n",
    "            self.e_channels = input_ch\n",
    "            self.e_height = image_size\n",
    "            self.e_width = image_size\n",
    "            self.e = torch.zeros(self.batch_size, self.e_channels, self.e_height, self.e_width, device=self.device)\n",
    "            e_flat_dim = self.e_channels * self.e_height * self.e_width\n",
    "            x1_flat_dim = (64 * scale) * self.image_size * self.image_size\n",
    "            self.Bc1 = nn.Parameter(torch.randn(e_flat_dim, x1_flat_dim), requires_grad=False)\n",
    "            self.Bc1_2 = nn.Parameter(torch.randn(e_flat_dim, x1_flat_dim), requires_grad=False)\n",
    "            x2_flat_dim = (128 * scale) * (self.image_size // 2) * (self.image_size // 2)\n",
    "            self.Bc2 = nn.Parameter(torch.randn(e_flat_dim, x2_flat_dim), requires_grad=False)\n",
    "            self.Bc2_2 = nn.Parameter(torch.randn(e_flat_dim, x2_flat_dim), requires_grad=False)\n",
    "            x3_flat_dim = (256 * scale) * (self.image_size // 4) * (self.image_size // 4)\n",
    "            self.Bc3 = nn.Parameter(torch.randn(e_flat_dim, x3_flat_dim), requires_grad=False)\n",
    "            self.Bc3_2 = nn.Parameter(torch.randn(e_flat_dim, x3_flat_dim), requires_grad=False)\n",
    "            x4_flat_dim = (128 * scale) * (self.image_size // 2) * (self.image_size // 2)\n",
    "            self.Bc4 = nn.Parameter(torch.randn(e_flat_dim, x4_flat_dim), requires_grad=False)\n",
    "            self.Bc4_2 = nn.Parameter(torch.randn(e_flat_dim, x4_flat_dim), requires_grad=False)\n",
    "    \n",
    "    def forward(self, x, timesteps, noise=None):\n",
    "        v = pos_encoding(timesteps, self.time_embed_dim, x.device)\n",
    "        x1_pre_inject = self.down1(x, v)\n",
    "        x1 = Inject_e.apply(x1_pre_inject, self.e, self.Bc1) if self.use_dfa else x1_pre_inject\n",
    "        x1_conv2 = self.down1_2(x1, v)\n",
    "        x1_conv2_injected = Inject_e.apply(x1_conv2, self.e, self.Bc1_2) if self.use_dfa else x1_conv2\n",
    "        x_down1_pooled = self.maxpool(x1_conv2_injected)\n",
    "        x2_pre_inject = self.down2(x_down1_pooled, v)\n",
    "        x2 = Inject_e.apply(x2_pre_inject, self.e, self.Bc2) if self.use_dfa else x2_pre_inject\n",
    "        x2_conv2 = self.down2_2(x2, v)\n",
    "        x2_conv2_injected = Inject_e.apply(x2_conv2, self.e, self.Bc2_2) if self.use_dfa else x2_conv2\n",
    "        x_down2_pooled = self.maxpool(x2_conv2_injected)\n",
    "        x_bot_pre_inject = self.bot1(x_down2_pooled, v)\n",
    "        x_bot = Inject_e.apply(x_bot_pre_inject, self.e, self.Bc3) if self.use_dfa else x_bot_pre_inject\n",
    "        x_bot2_pre_inject = self.bot1_2(x_bot, v)\n",
    "        x_bot2 = Inject_e.apply(x_bot2_pre_inject, self.e, self.Bc3_2) if self.use_dfa else x_bot2_pre_inject\n",
    "        x_up2_upsampled = self.upsample(x_bot2)\n",
    "        x_up2_concat = torch.cat([x_up2_upsampled, x2_conv2_injected], dim=1)\n",
    "        x_up2_pre_inject = self.up2(x_up2_concat, v)\n",
    "        x_up2 = Inject_e.apply(x_up2_pre_inject, self.e, self.Bc4) if self.use_dfa else x_up2_pre_inject\n",
    "        x_up2_conv2 = self.up2_2(x_up2, v)\n",
    "        x_up2_conv2_injected = Inject_e.apply(x_up2_conv2, self.e, self.Bc4_2) if self.use_dfa else x_up2_conv2\n",
    "        x_up1_upsampled = self.upsample(x_up2_conv2_injected)\n",
    "        x_up1_concat = torch.cat([x_up1_upsampled, x1_conv2_injected], dim=1)\n",
    "        x_final_conv1 = self.up1(x_up1_concat, v)\n",
    "        x_final_conv2 = self.up1_2(x_final_conv1, v)\n",
    "        output_noise_pred = self.out(x_final_conv2)\n",
    "        if self.use_dfa and noise is not None:\n",
    "            self.e.data.copy_(output_noise_pred - noise)\n",
    "        return output_noise_pred\n",
    "\n",
    "class Diffuser:\n",
    "    def __init__(self, num_timesteps=num_timesteps, beta_start=beta_start, beta_end=beta_end, device=device):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.device = device\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps, device=device)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "        self.schedulers = {}\n",
    "\n",
    "        # 1. DDPMScheduler\n",
    "        self.schedulers['ddpm'] = DDPMScheduler(\n",
    "            num_train_timesteps=self.num_timesteps,\n",
    "            beta_start=beta_start,\n",
    "            beta_end=beta_end,\n",
    "            beta_schedule='linear',\n",
    "            prediction_type='epsilon',\n",
    "            clip_sample=True, \n",
    "        )\n",
    "\n",
    "        # 2. DDIMScheduler (DPM-Solver-1)\n",
    "        self.schedulers['ddim'] =  DPMSolverMultistepScheduler(\n",
    "            num_train_timesteps=self.num_timesteps,\n",
    "            beta_start=beta_start,\n",
    "            beta_end=beta_end,\n",
    "            beta_schedule='linear',\n",
    "            algorithm_type=\"dpmsolver\",\n",
    "            solver_order=1,\n",
    "            final_sigmas_type=\"sigma_min\",\n",
    "            timestep_spacing='linspace',\n",
    "        )\n",
    "\n",
    "        # 3. DPM-Solver-2\n",
    "        self.schedulers['dpm_solver_2'] = DPMSolverMultistepScheduler(\n",
    "            num_train_timesteps=self.num_timesteps,\n",
    "            beta_start=beta_start,\n",
    "            beta_end=beta_end,\n",
    "            beta_schedule='linear',\n",
    "            algorithm_type=\"dpmsolver\",\n",
    "            solver_order=2,\n",
    "            final_sigmas_type=\"sigma_min\",\n",
    "            timestep_spacing='linspace',\n",
    "        )\n",
    "        \n",
    "        # 4. DPM-Solver++ (DPM2M++)\n",
    "        self.schedulers['dpm_solver_pp'] = DPMSolverMultistepScheduler(\n",
    "            num_train_timesteps=self.num_timesteps,\n",
    "            beta_start=beta_start,\n",
    "            beta_end=beta_end,\n",
    "            beta_schedule='linear',\n",
    "            algorithm_type=\"dpmsolver++\",\n",
    "            solver_order=2,\n",
    "            use_karras_sigmas=True,\n",
    "            timestep_spacing='linspace',\n",
    "        )\n",
    "        \n",
    "\n",
    "        print(f\"Diffuser initialized with schedulers: {list(self.schedulers.keys())}\")\n",
    "\n",
    "    def add_noise(self, x_0, t):\n",
    "        T = self.num_timesteps\n",
    "        assert (t >= 1).all() and (t <= T).all()\n",
    "        t_idx = t - 1\n",
    "        alpha_bar = self.alpha_bars[t_idx]\n",
    "        N = alpha_bar.size(0)\n",
    "        alpha_bar = alpha_bar.view(N, 1, 1, 1)\n",
    "        noise = torch.randn_like(x_0, device=self.device)\n",
    "        x_t = torch.sqrt(alpha_bar) * x_0 + torch.sqrt(1 - alpha_bar) * noise\n",
    "        return x_t, noise\n",
    "\n",
    "    def reverse_to_img(self, x):\n",
    "        x = x * 255\n",
    "        x = x.clamp(0, 255)\n",
    "        x = x.to(torch.uint8)\n",
    "        x = x.cpu()\n",
    "        to_pil = transforms.ToPILImage()\n",
    "        return to_pil(x)\n",
    "\n",
    "    def sample(self, model, sampler_type='ddpm', x_shape=(20, input_ch, image_size, image_size), num_sampling_steps=None, show_progress=True):        \n",
    "        if sampler_type not in self.schedulers:\n",
    "            print(f\"Warning: Sampler '{sampler_type}' not found. Defaulting to 'ddpm'.\")\n",
    "            sampler_type = 'ddpm'\n",
    "        scheduler = self.schedulers[sampler_type]\n",
    "        \n",
    "        if num_sampling_steps is None:\n",
    "            num_sampling_steps = SAMPLING_STEPS.get(sampler_type, self.num_timesteps)\n",
    "\n",
    "        batch_size = x_shape[0]\n",
    "        x = torch.randn(x_shape, device=self.device)\n",
    "        scheduler.set_timesteps(num_sampling_steps, device=self.device)\n",
    "        timesteps = scheduler.timesteps\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        desc = f\"Sampling ({sampler_type}, {num_sampling_steps} steps)\"\n",
    "        iterator = tqdm(timesteps, desc=desc) if show_progress else timesteps\n",
    "\n",
    "        for t in iterator:\n",
    "            t_input = torch.full((batch_size,), t.item(), device=self.device, dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                noise_pred = model(x, t_input, noise=None)\n",
    "            scheduler_output = scheduler.step(noise_pred, t, x, return_dict=False)\n",
    "            x = scheduler_output[0]\n",
    "\n",
    "        model.train()\n",
    "        images = [self.reverse_to_img(x[i]) for i in range(batch_size)]\n",
    "\n",
    "        return images\n",
    "\n",
    "def load_dataset(dataset_name, transform, class_to_keep=None):\n",
    "    print(f\"Loading {dataset_name} dataset...\")\n",
    "        \n",
    "    full_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    input_ch = 1\n",
    "    if class_to_keep is not None:\n",
    "        print(f\"Filtering MNIST for class: {class_to_keep}...\")\n",
    "        indices = [i for i, (_, label) in enumerate(full_dataset) if label == class_to_keep]\n",
    "        full_dataset = Subset(full_dataset, indices)\n",
    "        print(f\"Filtered dataset size: {len(full_dataset)}\")\n",
    "\n",
    "    print(f\"Total images loaded: {len(full_dataset)}, Input channels: {input_ch}\")\n",
    "\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "\n",
    "    if train_size == 0 or val_size == 0:\n",
    "        print(\"Dataset too small to split. Using all data for training.\")\n",
    "        return full_dataset, None, input_ch\n",
    "\n",
    "    print(f\"Splitting dataset into train ({train_size}) and validation ({val_size})...\")\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "    return train_dataset, val_dataset, input_ch\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- 2. preparing for dataset ---\")\n",
    "preprocess_tensor = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "try:\n",
    "    train_dataset, val_dataset, input_ch = load_dataset('mnist', preprocess_tensor, class_filter)\n",
    "    val_len = len(val_dataset) if val_dataset is not None else 0\n",
    "    print(f\"training images : ({len(train_dataset)}), validation images : ({val_len})\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "print(\"\\n--- 3. preparing for training ---\")\n",
    "if USE_DFA: method_name = \"DFA\"\n",
    "else: method_name = \"BP\"\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "run_name_base = f\"{selected_dataset}_UNet{model_version}_{method_name}_size{image_size}_batch{batch_size}_steps{num_timesteps}\"\n",
    "\n",
    "if class_filter is not None:\n",
    "    run_name = f\"{run_name_base}_class{class_filter}_{timestamp}\"\n",
    "else:\n",
    "    run_name = f\"{run_name_base}_{timestamp}\"\n",
    "    \n",
    "MODEL_SAVE_DIR = f\"./models/{run_name}\"\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "print(f\"Results will be saved in '{MODEL_SAVE_DIR}'\")\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=os.cpu_count() // 2 if os.cpu_count() else 0)\n",
    "\n",
    "diffuser = Diffuser(num_timesteps=num_timesteps, beta_start=beta_start, beta_end=beta_end, device=device)\n",
    "\n",
    "if model_version == 1:\n",
    "    model = UNet1(\n",
    "        input_ch=input_ch, time_embed_dim=time_embed_dim,\n",
    "        image_size=image_size, batch_size=batch_size,\n",
    "        device=device, use_dfa=USE_DFA\n",
    "    )\n",
    "else: # model_version == 2\n",
    "    model = UNet2(\n",
    "        input_ch=input_ch, time_embed_dim=time_embed_dim,\n",
    "        image_size=image_size, batch_size=batch_size,\n",
    "        device=device, use_dfa=USE_DFA\n",
    "    )\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n--- 4. Starting Training Loop ---\")\n",
    "print(f\"Dataset: {selected_dataset} (Class: {class_filter if class_filter is not None else 'All'})\")\n",
    "print(f\"Model: UNet{model_version} (scale: {scale})\")\n",
    "print(f\"Mode: {method_name}\")\n",
    "print(f\"Image Size: {image_size}x{image_size}\")\n",
    "print(f\"Train Timesteps: {num_timesteps}\")\n",
    "print(f\"Epochs: {epochs}, Batch Size: {batch_size}, LR: {lr}, Weight Decay: {weight_decay}\")\n",
    "print(f\"Visualization Sampler: {EVAL_SAMPLER_TYPE} @ {EVAL_SAMPLING_STEPS} steps\")\n",
    "print(f\"Results will be saved to: {MODEL_SAVE_DIR}\")\n",
    "print(f\"----------------------------------\")\n",
    "\n",
    "losses = []\n",
    "best_scores = {\n",
    "    'loss': {'score': float('inf'), 'epoch': -1}\n",
    "}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_sum = 0.0\n",
    "    cnt = 0\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch}/{epochs-1}: Sampling images for visualization...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            sampled_images_pil = diffuser.sample(model, \n",
    "                                                 sampler_type=EVAL_SAMPLER_TYPE,\n",
    "                                                 x_shape=(20, input_ch, image_size, image_size), \n",
    "                                                 num_sampling_steps=EVAL_SAMPLING_STEPS,\n",
    "                                                 show_progress=True)\n",
    "            \n",
    "            vis_dir = os.path.join(MODEL_SAVE_DIR, \"visualizations\")\n",
    "            os.makedirs(vis_dir, exist_ok=True)\n",
    "            vis_grid_tensor = torchvision.utils.make_grid([transforms.ToTensor()(img) for img in sampled_images_pil], nrow=4)\n",
    "            vis_pil = transforms.ToPILImage()(vis_grid_tensor)\n",
    "            vis_pil.save(os.path.join(vis_dir, f\"epoch_{epoch:04d}.png\"))\n",
    "            print(f\"Saved visualization grid to {vis_dir}/epoch_{epoch:04d}.png\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Visualization sampling failed: {e}\")\n",
    "    model.train()\n",
    "    \n",
    "\n",
    "    pbar_train = tqdm(dataloader, desc=f\"Epoch {epoch} Training\", leave=False)\n",
    "    for images, labels in pbar_train:\n",
    "        optimizer.zero_grad()\n",
    "        x = images.to(device)\n",
    "        t = torch.randint(1, num_timesteps + 1, (len(x),), device=device)\n",
    "\n",
    "        try:\n",
    "            x_noisy, noise = diffuser.add_noise(x, t)\n",
    "            noise_pred = model(x_noisy, t, noise if USE_DFA else None)\n",
    "\n",
    "            loss = F.mse_loss(noise, noise_pred)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_item = loss.item()\n",
    "            loss_sum += loss_item\n",
    "            cnt += 1\n",
    "            pbar_train.set_postfix(loss=f\"{loss_item:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during training step: {e}\")\n",
    "            continue\n",
    "\n",
    "    if cnt > 0:\n",
    "        loss_avg = loss_sum / cnt\n",
    "        losses.append(loss_avg)\n",
    "        print(f'Epoch {epoch} finished | Average Loss: {loss_avg:.4f}')\n",
    "        if loss_avg < best_scores['loss']['score']:\n",
    "            best_scores['loss']['score'] = loss_avg\n",
    "            best_scores['loss']['epoch'] = epoch\n",
    "            torch.save(model.state_dict(), os.path.join(MODEL_SAVE_DIR, \"best_loss_model.pth\"))\n",
    "            print(f\"New best loss model saved...\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch} had no successful batches.\")\n",
    "        losses.append(None)\n",
    "\n",
    "print(\"\\n--- 5. Training Finished ---\")\n",
    "print(f\"Results saved in {MODEL_SAVE_DIR}\")\n",
    "print(f\"Best Loss: {best_scores['loss']['score']:.4f} at Epoch {best_scores['loss']['epoch']}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 6. Generating Result Plots ---\")\n",
    "epochs_list = list(range(epochs))\n",
    "valid_losses = [l for l in losses if l is not None and l != float('inf')]\n",
    "valid_loss_epochs = [epochs_list[i] for i, l in enumerate(losses) if l is not None and l != float('inf')]\n",
    "\n",
    "\n",
    "if valid_losses:\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(valid_loss_epochs, valid_losses, label='Training Loss', marker='.')\n",
    "        plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"Training Loss Curve\\n{run_name}\")\n",
    "        plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "        save_filename = f\"{run_name}_loss_curve.png\"\n",
    "        plt.savefig(os.path.join(MODEL_SAVE_DIR, save_filename))\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate/save loss curve: {e}\")\n",
    "\n",
    "print(f\"\\nAll processes finished for {run_name}.\")\n",
    "print(f\"## {MODEL_SAVE_DIR}\")\n",
    "print(f\"losses:{losses}\")\n",
    "print(f\"Best Loss: {best_scores['loss']['score']:.4f} at Epoch {best_scores['loss']['epoch']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e88f4a-eb6c-4993-9bdc-494597de3adb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
