{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8342dd94-257b-42aa-a844-1d423ff18710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset, random_split\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import torch_fidelity\n",
    "from typing import List, Tuple\n",
    "from diffusers import DPMSolverMultistepScheduler, DDPMScheduler\n",
    "import numpy as np\n",
    "\n",
    "# 2. Hyperparameter\n",
    "image_size = 8\n",
    "batch_size = 8\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_timesteps = 250 # タイムステップ250用\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    "DPM_SOLVER_STEPS = 40\n",
    "scale = 3\n",
    "input_ch = 3\n",
    "time_embed_dim = 100\n",
    "epochs = 51\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-4\n",
    "eval_freq = 1\n",
    "num_eval_samples = 500\n",
    "REAL_DIR = \"./real_images\"\n",
    "GEN_DIR = \"./generated_images\"\n",
    "preprocess_tensor = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 3. for DFA\n",
    "class Inject_e(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, e, B):\n",
    "        ctx.save_for_backward(e, B)\n",
    "        ctx.grad_output_shape = x.shape\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        e, B = ctx.saved_tensors\n",
    "        target_grad_shape = ctx.grad_output_shape\n",
    "        e_flat = e.view(e.shape[0], -1)\n",
    "        grad_output_est = e_flat.mm(B)\n",
    "        grad_output_est = grad_output_est.view(target_grad_shape)\n",
    "        return grad_output_est, None, None\n",
    "\n",
    "# 4. Helper function\n",
    "def show_images(images, num_rows=3, num_cols=4, title=\"Generated Images\"):\n",
    "    if not images:\n",
    "        print(\"No images to display.\")\n",
    "        return\n",
    "    fig = plt.figure(figsize=(num_cols * 2, num_rows * 2))\n",
    "    plt.suptitle(title)\n",
    "    for i, img_pil in enumerate(images):\n",
    "        if i >= num_rows * num_cols:\n",
    "            break\n",
    "        ax = fig.add_subplot(num_rows, num_cols, i + 1)\n",
    "        ax.imshow(img_pil)\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "def _pos_encoding(time_idx, output_dim, device='cpu'):\n",
    "    t, D = time_idx, output_dim\n",
    "    v = torch.zeros(D, device=device)\n",
    "    i = torch.arange(0, D, device=device)\n",
    "    div_term = torch.exp(i / D * math.log(10000))\n",
    "    v[0::2] = torch.sin(t / div_term[0::2])\n",
    "    v[1::2] = torch.cos(t / div_term[1::2])\n",
    "    return v\n",
    "\n",
    "def pos_encoding(timesteps, output_dim, device='cpu'):\n",
    "    batch_size = len(timesteps)\n",
    "    device = timesteps.device\n",
    "    v = torch.zeros(batch_size, output_dim, device=device)\n",
    "    for i in range(batch_size):\n",
    "        v[i] = _pos_encoding(timesteps[i], output_dim, device)\n",
    "    return v\n",
    "\n",
    "# 5-1. U-Net1 architecture\n",
    "class ConvBlock1(nn.Module):\n",
    "    def __init__(self, input_ch, output_ch, time_embed_dim, use_dfa=True):\n",
    "        super().__init__()\n",
    "        activation = nn.Tanh() if use_dfa else nn.ReLU()\n",
    "        self.convs = nn.Sequential(nn.Conv2d(input_ch, output_ch, 5, padding=2), nn.BatchNorm2d(output_ch), activation)\n",
    "        self.mlp = nn.Sequential(nn.Linear(time_embed_dim, input_ch), activation, nn.Linear(input_ch, input_ch))\n",
    "    def forward(self, x, v):\n",
    "        N, C, _, _ = x.shape\n",
    "        v = self.mlp(v).view(N, C, 1, 1)\n",
    "        return self.convs(x + v)\n",
    "\n",
    "class UNet1(nn.Module):\n",
    "    def __init__(self, input_ch=input_ch, time_embed_dim=time_embed_dim, image_size=image_size, batch_size=batch_size, device='cpu', use_dfa=True):\n",
    "        super().__init__()\n",
    "        self.time_embed_dim = time_embed_dim\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.use_dfa = use_dfa\n",
    "        self.down1 = ConvBlock1(input_ch, 64 * scale, time_embed_dim)\n",
    "        self.down2 = ConvBlock1(64 * scale, 128 * scale, time_embed_dim)\n",
    "        self.bot1 = ConvBlock1(128 * scale, 256 * scale, time_embed_dim)\n",
    "        self.up2 = ConvBlock1(128 * scale + 256 * scale, 128 * scale, time_embed_dim)\n",
    "        self.up1 = ConvBlock1(128 * scale + 64 * scale, 64, time_embed_dim)\n",
    "        self.out = nn.Conv2d(64, input_ch, 1)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        if self.use_dfa:\n",
    "            self.e_channels = input_ch\n",
    "            self.e_height = image_size\n",
    "            self.e_width = image_size\n",
    "            self.e = torch.zeros(self.batch_size, self.e_channels, self.e_height, self.e_width, device=self.device)\n",
    "            e_flat_dim = self.e_channels * self.e_height * self.e_width\n",
    "            x1_flat_dim = (64 * scale) * self.image_size * self.image_size\n",
    "            self.Bc1 = nn.Parameter(torch.randn(e_flat_dim, x1_flat_dim), requires_grad=False)\n",
    "            x2_flat_dim = (128 * scale) * (self.image_size // 2) * (self.image_size // 2)\n",
    "            self.Bc2 = nn.Parameter(torch.randn(e_flat_dim, x2_flat_dim), requires_grad=False)\n",
    "            x3_flat_dim = (256 * scale) * (self.image_size // 4) * (self.image_size // 4)\n",
    "            self.Bc3 = nn.Parameter(torch.randn(e_flat_dim, x3_flat_dim), requires_grad=False)\n",
    "            x4_flat_dim = (128 * scale) * (self.image_size // 2) * (self.image_size // 2)\n",
    "            self.Bc4 = nn.Parameter(torch.randn(e_flat_dim, x4_flat_dim), requires_grad=False)\n",
    "    \n",
    "    def forward(self, x, timesteps, noise=None):\n",
    "        v = pos_encoding(timesteps, self.time_embed_dim, x.device)\n",
    "        x1_pre_inject = self.down1(x, v)\n",
    "        x1 = Inject_e.apply(x1_pre_inject, self.e, self.Bc1) if self.use_dfa else x1_pre_inject\n",
    "        x_down1_pooled = self.maxpool(x1)\n",
    "        x2_pre_inject = self.down2(x_down1_pooled, v)\n",
    "        x2 = Inject_e.apply(x2_pre_inject, self.e, self.Bc2) if self.use_dfa else x2_pre_inject\n",
    "        x_down2_pooled = self.maxpool(x2)\n",
    "        x_bot_pre_inject = self.bot1(x_down2_pooled, v)\n",
    "        x_bot = Inject_e.apply(x_bot_pre_inject, self.e, self.Bc3) if self.use_dfa else x_bot_pre_inject\n",
    "        x_up2_upsampled = self.upsample(x_bot)\n",
    "        x_up2_concat = torch.cat([x_up2_upsampled, x2], dim=1)\n",
    "        x_up2_pre_inject = self.up2(x_up2_concat, v)\n",
    "        x_up2 = Inject_e.apply(x_up2_pre_inject, self.e, self.Bc4) if self.use_dfa else x_up2_pre_inject\n",
    "        x_up1_upsampled = self.upsample(x_up2)\n",
    "        x_up1_concat = torch.cat([x_up1_upsampled, x1], dim=1)\n",
    "        x_final_conv = self.up1(x_up1_concat, v)\n",
    "        output_noise_pred = self.out(x_final_conv)\n",
    "        if self.use_dfa and noise is not None:\n",
    "            self.e.data.copy_(output_noise_pred - noise)\n",
    "        return output_noise_pred\n",
    "\n",
    "# 5-2. U-Net2 architecture\n",
    "class ConvBlock2(nn.Module):\n",
    "    def __init__(self, input_ch, output_ch, time_embed_dim, use_dfa=True):\n",
    "        super().__init__()\n",
    "        activation = nn.Tanh() if use_dfa else nn.ReLU()\n",
    "        self.convs = nn.Sequential(nn.Conv2d(input_ch, output_ch, 3, padding=1), nn.BatchNorm2d(output_ch), activation)\n",
    "        self.mlp = nn.Sequential(nn.Linear(time_embed_dim, input_ch), activation, nn.Linear(input_ch, input_ch))\n",
    "    def forward(self, x, v):\n",
    "        N, C, _, _ = x.shape\n",
    "        v = self.mlp(v).view(N, C, 1, 1)\n",
    "        return self.convs(x + v)\n",
    "\n",
    "class UNet2(nn.Module):\n",
    "    def __init__(self, input_ch=input_ch, time_embed_dim=time_embed_dim, image_size=image_size, batch_size=batch_size, device='cpu', use_dfa=True):\n",
    "        super().__init__()\n",
    "        self.time_embed_dim = time_embed_dim\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.use_dfa = use_dfa\n",
    "        self.down1 = ConvBlock2(input_ch, 64 * scale, time_embed_dim)\n",
    "        self.down1_2 = ConvBlock2(64 * scale, 64 * scale, time_embed_dim)\n",
    "        self.down2 = ConvBlock2(64 * scale, 128 * scale, time_embed_dim)\n",
    "        self.down2_2 = ConvBlock2(128 * scale, 128 * scale, time_embed_dim)\n",
    "        self.bot1 = ConvBlock2(128 * scale, 256 * scale, time_embed_dim)\n",
    "        self.bot1_2 = ConvBlock2(256 * scale, 256 * scale, time_embed_dim)\n",
    "        self.up2 = ConvBlock2(128 * scale + 256 * scale, 128 * scale, time_embed_dim)\n",
    "        self.up2_2 = ConvBlock2(128 * scale, 128 * scale, time_embed_dim)\n",
    "        self.up1 = ConvBlock2(128 * scale + 64 * scale, 64, time_embed_dim)\n",
    "        self.up1_2 = ConvBlock2(64, 64, time_embed_dim)\n",
    "        self.out = nn.Conv2d(64, input_ch, 1)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        if self.use_dfa:\n",
    "            self.e_channels = input_ch\n",
    "            self.e_height = image_size\n",
    "            self.e_width = image_size\n",
    "            self.e = torch.zeros(self.batch_size, self.e_channels, self.e_height, self.e_width, device=self.device)\n",
    "            e_flat_dim = self.e_channels * self.e_height * self.e_width\n",
    "            x1_flat_dim = (64 * scale) * self.image_size * self.image_size\n",
    "            self.Bc1 = nn.Parameter(torch.randn(e_flat_dim, x1_flat_dim), requires_grad=False)\n",
    "            self.Bc1_2 = nn.Parameter(torch.randn(e_flat_dim, x1_flat_dim), requires_grad=False)\n",
    "            x2_flat_dim = (128 * scale) * (self.image_size // 2) * (self.image_size // 2)\n",
    "            self.Bc2 = nn.Parameter(torch.randn(e_flat_dim, x2_flat_dim), requires_grad=False)\n",
    "            self.Bc2_2 = nn.Parameter(torch.randn(e_flat_dim, x2_flat_dim), requires_grad=False)\n",
    "            x3_flat_dim = (256 * scale) * (self.image_size // 4) * (self.image_size // 4)\n",
    "            self.Bc3 = nn.Parameter(torch.randn(e_flat_dim, x3_flat_dim), requires_grad=False)\n",
    "            self.Bc3_2 = nn.Parameter(torch.randn(e_flat_dim, x3_flat_dim), requires_grad=False)\n",
    "            x4_flat_dim = (128 * scale) * (self.image_size // 2) * (self.image_size // 2)\n",
    "            self.Bc4 = nn.Parameter(torch.randn(e_flat_dim, x4_flat_dim), requires_grad=False)\n",
    "            self.Bc4_2 = nn.Parameter(torch.randn(e_flat_dim, x4_flat_dim), requires_grad=False)\n",
    "    \n",
    "    def forward(self, x, timesteps, noise=None):\n",
    "        v = pos_encoding(timesteps, self.time_embed_dim, x.device)\n",
    "        x1_pre_inject = self.down1(x, v)\n",
    "        x1 = Inject_e.apply(x1_pre_inject, self.e, self.Bc1) if self.use_dfa else x1_pre_inject\n",
    "        x1_conv2 = self.down1_2(x1, v)\n",
    "        x1_conv2_injected = Inject_e.apply(x1_conv2, self.e, self.Bc1_2) if self.use_dfa else x1_conv2\n",
    "        x_down1_pooled = self.maxpool(x1_conv2_injected)\n",
    "        x2_pre_inject = self.down2(x_down1_pooled, v)\n",
    "        x2 = Inject_e.apply(x2_pre_inject, self.e, self.Bc2) if self.use_dfa else x2_pre_inject\n",
    "        x2_conv2 = self.down2_2(x2, v)\n",
    "        x2_conv2_injected = Inject_e.apply(x2_conv2, self.e, self.Bc2_2) if self.use_dfa else x2_conv2\n",
    "        x_down2_pooled = self.maxpool(x2_conv2_injected)\n",
    "        x_bot_pre_inject = self.bot1(x_down2_pooled, v)\n",
    "        x_bot = Inject_e.apply(x_bot_pre_inject, self.e, self.Bc3) if self.use_dfa else x_bot_pre_inject\n",
    "        x_bot2_pre_inject = self.bot1_2(x_bot, v)\n",
    "        x_bot2 = Inject_e.apply(x_bot2_pre_inject, self.e, self.Bc3_2) if self.use_dfa else x_bot2_pre_inject\n",
    "        x_up2_upsampled = self.upsample(x_bot2)\n",
    "        x_up2_concat = torch.cat([x_up2_upsampled, x2_conv2_injected], dim=1)\n",
    "        x_up2_pre_inject = self.up2(x_up2_concat, v)\n",
    "        x_up2 = Inject_e.apply(x_up2_pre_inject, self.e, self.Bc4) if self.use_dfa else x_up2_pre_inject\n",
    "        x_up2_conv2 = self.up2_2(x_up2, v)\n",
    "        x_up2_conv2_injected = Inject_e.apply(x_up2_conv2, self.e, self.Bc4_2) if self.use_dfa else x_up2_conv2\n",
    "        x_up1_upsampled = self.upsample(x_up2_conv2_injected)\n",
    "        x_up1_concat = torch.cat([x_up1_upsampled, x1_conv2_injected], dim=1)\n",
    "        x_final_conv1 = self.up1(x_up1_concat, v)\n",
    "        x_final_conv2 = self.up1_2(x_final_conv1, v)\n",
    "        output_noise_pred = self.out(x_final_conv2)\n",
    "        if self.use_dfa and noise is not None:\n",
    "            self.e.data.copy_(output_noise_pred - noise)\n",
    "        return output_noise_pred\n",
    "\n",
    "# 6. Diffusers\n",
    "class Diffuser:\n",
    "    def __init__(self, num_timesteps=num_timesteps, beta_start=beta_start, beta_end=beta_end, device=device):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.device = device\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps, device=device)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "        self.schedulers = {}\n",
    "\n",
    "        # 6-1. DDPMScheduler\n",
    "        self.schedulers['ddpm'] = DDPMScheduler(\n",
    "            num_train_timesteps=self.num_timesteps,\n",
    "            beta_start=beta_start,\n",
    "            beta_end=beta_end,\n",
    "            beta_schedule='linear',\n",
    "            prediction_type='epsilon',\n",
    "            clip_sample=True, \n",
    "        )\n",
    "\n",
    "        # 6-2. DDIMScheduler (DPM-Solver-1)\n",
    "        self.schedulers['ddim'] =  DPMSolverMultistepScheduler(\n",
    "            num_train_timesteps=self.num_timesteps,\n",
    "            beta_start=beta_start,\n",
    "            beta_end=beta_end,\n",
    "            beta_schedule='linear',\n",
    "            algorithm_type=\"dpmsolver\",\n",
    "            solver_order=1,\n",
    "            final_sigmas_type=\"sigma_min\",\n",
    "            timestep_spacing='linspace',\n",
    "        )\n",
    "\n",
    "        # 6-3. DPM-Solver-2\n",
    "        self.schedulers['dpm_solver_2'] = DPMSolverMultistepScheduler(\n",
    "            num_train_timesteps=self.num_timesteps,\n",
    "            beta_start=beta_start,\n",
    "            beta_end=beta_end,\n",
    "            beta_schedule='linear',\n",
    "            algorithm_type=\"dpmsolver\",\n",
    "            solver_order=2,\n",
    "            final_sigmas_type=\"sigma_min\",\n",
    "            timestep_spacing='linspace',\n",
    "        )\n",
    "        \n",
    "        # 6-4. DPM-Solver++ (DPM2M++)\n",
    "        self.schedulers['dpm_solver_pp'] = DPMSolverMultistepScheduler(\n",
    "            num_train_timesteps=self.num_timesteps,\n",
    "            beta_start=beta_start,\n",
    "            beta_end=beta_end,\n",
    "            beta_schedule='linear',\n",
    "            algorithm_type=\"dpmsolver++\",\n",
    "            solver_order=2,\n",
    "            use_karras_sigmas=True,\n",
    "            timestep_spacing='linspace',\n",
    "        )\n",
    "        \n",
    "\n",
    "        print(f\"Diffuser initialized with schedulers: {list(self.schedulers.keys())}\")\n",
    "\n",
    "    def add_noise(self, x_0, t):\n",
    "        T = self.num_timesteps\n",
    "        assert (t >= 1).all() and (t <= T).all()\n",
    "        t_idx = t - 1\n",
    "        alpha_bar = self.alpha_bars[t_idx]\n",
    "        N = alpha_bar.size(0)\n",
    "        alpha_bar = alpha_bar.view(N, 1, 1, 1)\n",
    "        noise = torch.randn_like(x_0, device=self.device)\n",
    "        x_t = torch.sqrt(alpha_bar) * x_0 + torch.sqrt(1 - alpha_bar) * noise\n",
    "        return x_t, noise\n",
    "\n",
    "    def reverse_to_img(self, x):\n",
    "        x = x * 255\n",
    "        x = x.clamp(0, 255)\n",
    "        x = x.to(torch.uint8)\n",
    "        x = x.cpu()\n",
    "        to_pil = transforms.ToPILImage()\n",
    "        return to_pil(x)\n",
    "\n",
    "    def sample(self, model, sampler_type='ddpm', x_shape=(20, input_ch, image_size, image_size), num_sampling_steps=None, show_progress=True):        \n",
    "        if sampler_type not in self.schedulers:\n",
    "            print(f\"Warning: Sampler '{sampler_type}' not found. Defaulting to 'ddpm'.\")\n",
    "            sampler_type = 'ddpm'\n",
    "        scheduler = self.schedulers[sampler_type]\n",
    "        \n",
    "        if num_sampling_steps is None:\n",
    "            num_sampling_steps = SAMPLING_STEPS.get(sampler_type, self.num_timesteps)\n",
    "\n",
    "        batch_size = x_shape[0]\n",
    "        x = torch.randn(x_shape, device=self.device)\n",
    "        scheduler.set_timesteps(num_sampling_steps, device=self.device)\n",
    "        timesteps = scheduler.timesteps\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        desc = f\"Sampling ({sampler_type}, {num_sampling_steps} steps)\"\n",
    "        iterator = tqdm(timesteps, desc=desc) if show_progress else timesteps\n",
    "\n",
    "        for t in iterator:\n",
    "            t_input = torch.full((batch_size,), t.item(), device=self.device, dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                noise_pred = model(x, t_input, noise=None)\n",
    "            scheduler_output = scheduler.step(noise_pred, t, x, return_dict=False)\n",
    "            x = scheduler_output[0]\n",
    "\n",
    "        model.train()\n",
    "        images = [self.reverse_to_img(x[i]) for i in range(batch_size)]\n",
    "\n",
    "        return images\n",
    "\n",
    "\n",
    "# 7. Training\n",
    "# --- Select num_timesteps ---\n",
    "num_timesteps = None\n",
    "while num_timesteps is None:\n",
    "    try:\n",
    "        timesteps_input = int(input(\"Select num_timesteps (e.g. 250, 500, 750, 1000): \").strip())\n",
    "        if timesteps_input > 0:\n",
    "            num_timesteps = timesteps_input\n",
    "            print(f\"num_timesteps selected: {num_timesteps}\")\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter a positive number.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "# --- Select Training Mode (DFA/BP) ---\n",
    "USE_DFA = None\n",
    "while USE_DFA is None:\n",
    "    mode_input = input(\"Select training mode (DFA/BP): \").strip().lower()\n",
    "    if mode_input == 'dfa': USE_DFA = True\n",
    "    elif mode_input == 'bp': USE_DFA = False\n",
    "    else: print(\"Invalid input. Please type 'DFA' or 'BP'.\")\n",
    "batch_size = 8 if USE_DFA else 64\n",
    "scale = 3 if USE_DFA else 1\n",
    "print(f\"Batch size set to: {batch_size}, scale set to: {scale}\")\n",
    "\n",
    "# --- Select U-Net version ---\n",
    "model_version = None\n",
    "while model_version is None:\n",
    "    model_input = input(\"Select UNet version (1/2): \").strip()\n",
    "    if model_input == '1':\n",
    "        model_version = 1\n",
    "        print(\"UNet version selected: UNet1 (1 ConvBlock per layer)\")\n",
    "    elif model_input == '2':\n",
    "        model_version = 2\n",
    "        print(\"UNet version selected: UNet2 (2 ConvBlocks per layer)\")\n",
    "    else:\n",
    "        print(\"Invalid input. Please type '1' or '2'.\")\n",
    "\n",
    "# --- Select image_size ---\n",
    "image_size = None\n",
    "while image_size is None:\n",
    "    try:\n",
    "        size_input = int(input(\"Select image size (e.g., 8, 16, 32, 64): \").strip())\n",
    "        if size_input > 0:\n",
    "            image_size = size_input\n",
    "            print(f\"Image size selected: {image_size}\")\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter a positive number.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "# --- Select Sampler  ---\n",
    "sampler_type = None\n",
    "sampling_steps = DPM_SOLVER_STEPS # Default 40\n",
    "\n",
    "while sampler_type is None:\n",
    "    print(\"\\nSelect Sampler for Evaluation:\")\n",
    "    print(\"1: DDPM (Slow but standard)\")\n",
    "    print(\"2: DDIM (DPM-Solver-1)\")\n",
    "    print(\"3: DPM-Solver-2\")\n",
    "    print(\"4: DPM-Solver++\")\n",
    "    \n",
    "    s_input = input(\"Enter choice (1-4): \").strip()\n",
    "    \n",
    "    if s_input == '1':\n",
    "        sampler_type = 'ddpm'\n",
    "        sampling_steps = num_timesteps # DDPMは全ステップ推奨\n",
    "    elif s_input == '2':\n",
    "        sampler_type = 'ddim'\n",
    "    elif s_input == '3':\n",
    "        sampler_type = 'dpm_solver_2'\n",
    "    elif s_input == '4':\n",
    "        sampler_type = 'dpm_solver_pp'\n",
    "    else:\n",
    "        print(\"Invalid input.\")\n",
    "        \n",
    "print(f\"Sampler selected: {sampler_type}, Sampling steps: {sampling_steps}\")\n",
    "\n",
    "\n",
    "# --- Load dataset, combine, split ---\n",
    "print(\"\\nLoading Oxford Flowers 102 dataset (all splits: train, val, test) and combining them...\")\n",
    "try:\n",
    "    original_train_ds = torchvision.datasets.Flowers102(root='./data', split='train', download=True, transform=preprocess_tensor)\n",
    "    original_val_ds = torchvision.datasets.Flowers102(root='./data', split='val', download=True, transform=preprocess_tensor)\n",
    "    original_test_ds = torchvision.datasets.Flowers102(root='./data', split='test', download=True, transform=preprocess_tensor)\n",
    "    full_combined_dataset = ConcatDataset([original_train_ds, original_val_ds, original_test_ds])\n",
    "    print(f\"Total images: {len(full_combined_dataset)}\")\n",
    "    train_size = int(0.8 * len(full_combined_dataset))\n",
    "    val_size = len(full_combined_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_combined_dataset, [train_size, val_size],\n",
    "                                              generator=torch.Generator().manual_seed(42))\n",
    "    print(f\"Training : ({len(train_dataset)}), Validation({len(val_dataset)})\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Flowers102 dataset: {e}\")\n",
    "    try:\n",
    "        train_dataset = torchvision.datasets.Flowers102(root='./data', download=True, transform=preprocess_tensor)\n",
    "        val_dataset = None\n",
    "        full_combined_dataset = train_dataset\n",
    "    except Exception as e_retry:\n",
    "        print(f\"Fatal: Could not load Flowers102 dataset even with default settings: {e_retry}\")\n",
    "        sys.exit()\n",
    "\n",
    "# --- Setting for the saving directory ---\n",
    "if USE_DFA:\n",
    "    method_name = \"DFA\"\n",
    "else:\n",
    "    method_name = \"BP\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "run_name = f\"UNet{model_version}_{method_name}_train timestep{num_timesteps}_size{image_size}_batch{batch_size}_{timestamp}\"\n",
    "MODEL_SAVE_DIR = f\"./models/{run_name}\"\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "print(f\"Results will be saved in '{MODEL_SAVE_DIR}' \")\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=os.cpu_count() // 2 if os.cpu_count() else 0)\n",
    "\n",
    "diffuser = Diffuser(num_timesteps, device=device)\n",
    "\n",
    "if model_version == 1:\n",
    "    model = UNet1(\n",
    "        input_ch=input_ch,\n",
    "        time_embed_dim=time_embed_dim,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        use_dfa=USE_DFA\n",
    "    )\n",
    "else: # model_version == 2\n",
    "    model = UNet2(\n",
    "        input_ch=input_ch,\n",
    "        time_embed_dim=time_embed_dim,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        use_dfa=USE_DFA\n",
    "    )\n",
    "\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "REAL_DIR = f\"./real_images_size{image_size}\"\n",
    "real_images_for_eval_source = val_dataset if val_dataset else train_dataset\n",
    "actual_num_real_eval_samples = min(num_eval_samples, len(real_images_for_eval_source))\n",
    "\n",
    "print(f\"\\n Saving for torch-fidelity (from Validation dataset : {actual_num_real_eval_samples})\")\n",
    "if not os.path.exists(REAL_DIR) or len(os.listdir(REAL_DIR)) < actual_num_real_eval_samples:\n",
    "    print(f\"Saving {actual_num_real_eval_samples} real images for calculation...\")\n",
    "    if os.path.exists(REAL_DIR):\n",
    "        shutil.rmtree(REAL_DIR)\n",
    "    os.makedirs(REAL_DIR, exist_ok=True)\n",
    "\n",
    "    real_images_saved_count = 0\n",
    "    temp_dataloader_for_real = DataLoader(real_images_for_eval_source, batch_size=1, shuffle=True)\n",
    "    for img_tensor, _ in tqdm(temp_dataloader_for_real, desc=\"Saving real images\"):\n",
    "        if real_images_saved_count >= actual_num_real_eval_samples:\n",
    "            break\n",
    "        img_pil = transforms.ToPILImage()(img_tensor.squeeze(0))\n",
    "        img_pil.save(os.path.join(REAL_DIR, f\"real_{real_images_saved_count:05d}.png\"))\n",
    "        real_images_saved_count += 1\n",
    "    print(f\"{real_images_saved_count} real images saved to {REAL_DIR}\")\n",
    "else:\n",
    "    print(f\"Enough real images already exist in {REAL_DIR}. Skipping saving.\")\n",
    "\n",
    "print(f\"scale : {scale}, batch_size : {batch_size}, learning_rate : {lr}, weight_decay : {weight_decay}\")\n",
    "\n",
    "losses = []\n",
    "fid_scores = []\n",
    "kid_scores = []\n",
    "is_scores = []\n",
    "\n",
    "best_scores = {\n",
    "    'fid': {'score': float('inf'), 'epoch': -1},\n",
    "    'kid': {'score': float('inf'), 'epoch': -1},\n",
    "    'is': {'score': float('-inf'), 'epoch': -1},\n",
    "    'loss': {'score': float('inf'), 'epoch': -1}\n",
    "}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_sum = 0.0\n",
    "    cnt = 0\n",
    "    \n",
    "    # 修正: 選択したサンプラーとステップ数を使用\n",
    "    print(f\"\\nEpoch {epoch}: Sampling images for visualization ({sampler_type}, {sampling_steps} steps)...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sampled_images = diffuser.sample(model, x_shape=(20, input_ch, image_size, image_size), \n",
    "                                         sampler_type=sampler_type, # 修正\n",
    "                                         num_sampling_steps=sampling_steps, # 修正\n",
    "                                         show_progress=True)\n",
    "        show_images(sampled_images, title=f\"Epoch {epoch} Generated Images ({sampler_type.upper()})\")\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    if (epoch + 1) % eval_freq == 0:\n",
    "        print(f\"Epoch {epoch}: Calculating FID, KID, IS...\")\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        GENERATED_DIR_RUN = os.path.join(MODEL_SAVE_DIR, f\"generated_images_epoch_{epoch}\")\n",
    "        if os.path.exists(GENERATED_DIR_RUN):\n",
    "            shutil.rmtree(GENERATED_DIR_RUN)\n",
    "        os.makedirs(GENERATED_DIR_RUN, exist_ok=True)\n",
    "\n",
    "        generated_images_count = 0\n",
    "        with tqdm(total=num_eval_samples, desc=\"Generating & Saving for evaluation\", unit=\"img\") as pbar:\n",
    "            num_to_generate_per_call = 20\n",
    "            num_sample_calls = math.ceil(num_eval_samples / num_to_generate_per_call)\n",
    "\n",
    "            for _ in range(num_sample_calls):\n",
    "                if generated_images_count >= num_eval_samples:\n",
    "                    break\n",
    "                \n",
    "                # 修正: 選択したサンプラーとステップ数を使用\n",
    "                gen_batch_pil = diffuser.sample(model, x_shape=(num_to_generate_per_call, input_ch, image_size, image_size), \n",
    "                                                sampler_type=sampler_type, # 修正\n",
    "                                                num_sampling_steps=sampling_steps, # 修正\n",
    "                                                show_progress=False)\n",
    "\n",
    "                for img_pil in gen_batch_pil:\n",
    "                    if generated_images_count >= num_eval_samples:\n",
    "                        break\n",
    "                    img_pil.save(os.path.join(GENERATED_DIR_RUN, f\"gen_{generated_images_count:05d}.png\"))\n",
    "                    generated_images_count += 1\n",
    "                    pbar.update(1)\n",
    "\n",
    "        print(f\"{generated_images_count} generated images saved to {GENERATED_DIR_RUN}\")\n",
    "\n",
    "        try:\n",
    "            metrics_dict = torch_fidelity.calculate_metrics(\n",
    "                input1=GENERATED_DIR_RUN,\n",
    "                input2=REAL_DIR,\n",
    "                cuda=True,\n",
    "                fid=True,\n",
    "                kid=True,\n",
    "                isc=True,\n",
    "                prc=False,\n",
    "                kid_subset_size=num_eval_samples\n",
    "            )\n",
    "\n",
    "            fid_score = metrics_dict['frechet_inception_distance']\n",
    "            kid_score = metrics_dict['kernel_inception_distance_mean']\n",
    "            is_score = metrics_dict['inception_score_mean']\n",
    "\n",
    "            print(f\"Epoch {epoch} | FID: {fid_score:.4f} | KID: {kid_score:.4f} | IS: {is_score:.4f}\")\n",
    "\n",
    "            fid_scores.append(fid_score)\n",
    "            kid_scores.append(kid_score)\n",
    "            is_scores.append(is_score)\n",
    "            \n",
    "            current_eval_epoch = epoch\n",
    "            \n",
    "            \n",
    "            if kid_score < best_scores['kid']['score']:\n",
    "                best_scores['kid']['score'] = kid_score\n",
    "                best_scores['kid']['epoch'] = current_eval_epoch\n",
    "                torch.save(model.state_dict(), os.path.join(MODEL_SAVE_DIR, \"best_kid_model.pth\"))\n",
    "                print(f\"New best KID model saved with KID: {best_scores['kid']['score']:.4f} at Epoch {current_eval_epoch}\")\n",
    "\n",
    "            if fid_score < best_scores['fid']['score']:\n",
    "                best_scores['fid']['score'] = fid_score\n",
    "                best_scores['fid']['epoch'] = current_eval_epoch\n",
    "                torch.save(model.state_dict(), os.path.join(MODEL_SAVE_DIR, \"best_fid_model.pth\"))\n",
    "                print(f\"New best FID model saved with FID: {best_scores['fid']['score']:.4f} at Epoch {current_eval_epoch}\")\n",
    "\n",
    "            if is_score > best_scores['is']['score']:\n",
    "                best_scores['is']['score'] = is_score\n",
    "                best_scores['is']['epoch'] = current_eval_epoch\n",
    "                torch.save(model.state_dict(), os.path.join(MODEL_SAVE_DIR, \"best_is_model.pth\"))\n",
    "                print(f\"New best IS model saved with IS: {best_scores['is']['score']:.4f} at Epoch {current_eval_epoch}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"FID/KID/IS calculation failed: {e}\")\n",
    "            print(\"Make sure you have enough samples and torch-fidelity is correctly installed.\")\n",
    "            fid_scores.append(None)\n",
    "            kid_scores.append(None)\n",
    "            is_scores.append(None)\n",
    "\n",
    "        shutil.rmtree(GENERATED_DIR_RUN)\n",
    "        model.train()\n",
    "\n",
    "\n",
    "    for images, labels in tqdm(dataloader, desc=f\"Epoch {epoch} Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        x = images.to(device)\n",
    "        t = torch.randint(1, num_timesteps + 1, (len(x),), device=device)\n",
    "\n",
    "        x_noisy, noise = diffuser.add_noise(x, t)\n",
    "        noise_pred = model(x_noisy, t, noise)\n",
    "\n",
    "        loss = F.mse_loss(noise, noise_pred)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        cnt += 1\n",
    "\n",
    "    loss_avg = loss_sum / cnt\n",
    "    if loss_avg < best_scores['loss']['score']:\n",
    "        best_scores['loss']['score'] = loss_avg\n",
    "        best_scores['loss']['epoch'] = epoch + 1\n",
    "        torch.save(model.state_dict(), os.path.join(MODEL_SAVE_DIR, \"best_loss_model.pth\"))\n",
    "    losses.append(loss_avg)\n",
    "    print(f'Epoch {epoch} | Loss: {loss_avg}')\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# 8. Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "epochs_evaluated_fid_kid_is = [(i + 0) for i in range(epochs) if (i + 1) % eval_freq == 0] \n",
    "\n",
    "best_scores = {\n",
    "    'loss': {'score': float('inf'), 'epoch': -1}, # min\n",
    "    'fid': {'score': float('inf'), 'epoch': -1},  # min\n",
    "    'kid': {'score': float('inf'), 'epoch': -1},  # min\n",
    "    'is': {'score': float('-inf'), 'epoch': -1}   # max\n",
    "}\n",
    "\n",
    "# Loss\n",
    "min_loss_idx = np.argmin(losses)\n",
    "best_scores['loss']['score'] = losses[min_loss_idx]\n",
    "best_scores['loss']['epoch'] = min_loss_idx \n",
    "\n",
    "# FID\n",
    "min_fid_idx = np.argmin(fid_scores)\n",
    "best_scores['fid']['score'] = fid_scores[min_fid_idx]\n",
    "best_scores['fid']['epoch'] = epochs_evaluated_fid_kid_is[min_fid_idx]\n",
    "\n",
    "# KID\n",
    "min_kid_idx = np.argmin(kid_scores)\n",
    "best_scores['kid']['score'] = kid_scores[min_kid_idx]\n",
    "best_scores['kid']['epoch'] = epochs_evaluated_fid_kid_is[min_kid_idx]\n",
    "\n",
    "# IS\n",
    "max_is_idx = np.argmax(is_scores)\n",
    "best_scores['is']['score'] = is_scores[max_is_idx]\n",
    "best_scores['is']['epoch'] = epochs_evaluated_fid_kid_is[max_is_idx]\n",
    "\n",
    "def plot_scores_with_annotations(\n",
    "    epochs_list: List[int], \n",
    "    scores: List[float],\n",
    "    best_info: dict,\n",
    "    title: str,\n",
    "    ylabel: str,\n",
    "    metric_type: str,\n",
    "    color: str\n",
    "):\n",
    "   \n",
    "    valid_data = [(e, s) for e, s in zip(epochs_list, scores) if s is not None]\n",
    "    if not valid_data:\n",
    "        print(f\"No valid data to plot for {ylabel}.\")\n",
    "        return\n",
    "\n",
    "    valid_epochs = [d[0] for d in valid_data]\n",
    "    valid_scores = [d[1] for d in valid_data]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(valid_epochs, valid_scores, label=ylabel, color=color, marker='o', markersize=5, linestyle='-')\n",
    "\n",
    "    best_epoch_idx = best_info['epoch'] \n",
    "\n",
    "    best_score = best_info['score']\n",
    "    \n",
    "    plot_epoch = best_epoch_idx \n",
    "\n",
    "    if best_epoch_idx != -1:\n",
    "        \n",
    "        plt.scatter(plot_epoch, best_score, color='red', s=150, zorder=5, label=f'Best ({metric_type.capitalize()})', marker='*')\n",
    "\n",
    "        best_text = f'{metric_type.capitalize()}: {best_score:.4f}\\n(Epoch {plot_epoch})' \n",
    "\n",
    "        std_scores = np.std(valid_scores)\n",
    "        if metric_type == 'min':\n",
    "            text_y_pos = best_score + std_scores * 0.5\n",
    "            if text_y_pos > max(valid_scores):\n",
    "                text_y_pos = best_score - std_scores * 0.5\n",
    "        else: # metric_type == 'max'\n",
    "            text_y_pos = best_score - std_scores * 0.5\n",
    "            if text_y_pos < min(valid_scores):\n",
    "                text_y_pos = best_score + std_scores * 0.5\n",
    "\n",
    "        plt.annotate(\n",
    "            best_text,\n",
    "            xy=(plot_epoch, best_score),\n",
    "            xytext=(plot_epoch, text_y_pos),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05, width=0.5, headwidth=8),\n",
    "            fontsize=10,\n",
    "            ha='center',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7)\n",
    "        )\n",
    "\n",
    "    plt.title(title, fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Epoch', fontsize=10)\n",
    "    plt.ylabel(ylabel, fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    all_epochs = valid_epochs + [best_epoch_idx]\n",
    "    tick_step = max(1, len(valid_epochs) // 10)\n",
    "    plt.xticks(sorted(list(set([e for e in all_epochs if e % tick_step == 0] + [best_epoch_idx]))))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(MODEL_SAVE_DIR, f\"{ylabel.lower().replace(' ', '_')}_curve.png\"))\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([(e) for e in range(len(losses))], losses, label='Training Loss') \n",
    "plt.scatter(best_scores['loss']['epoch'], best_scores['loss']['score'], color='red', s=150, zorder=5, label='Min Loss', marker='*')\n",
    "if best_scores['loss']['epoch'] != -1 and len(losses) > 1:\n",
    "    plt.annotate(\n",
    "        f'Min Loss: {best_scores[\"loss\"][\"score\"]:.4f}\\n(Epoch {best_scores[\"loss\"][\"epoch\"]})', \n",
    "        xy=(best_scores['loss']['epoch'], best_scores['loss']['score']),\n",
    "        xytext=(best_scores['loss']['epoch'], best_scores['loss']['score'] + np.std(losses) * 0.5),\n",
    "        arrowprops=dict(facecolor='black', shrink=0.05, width=0.5, headwidth=8),\n",
    "        fontsize=10,\n",
    "        ha='center',\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7)\n",
    "    )\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(f\"Training Loss Curve for {run_name}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(MODEL_SAVE_DIR, \"loss_curve.png\"))\n",
    "plt.show()\n",
    "\n",
    "# FID\n",
    "plot_scores_with_annotations(\n",
    "    epochs_evaluated_fid_kid_is, fid_scores, best_scores['fid'],\n",
    "    f'FID Score (Lower is Better) for {run_name}', 'FID', 'min', 'tab:blue'\n",
    ")\n",
    "\n",
    "# KID\n",
    "plot_scores_with_annotations(\n",
    "    epochs_evaluated_fid_kid_is, kid_scores, best_scores['kid'],\n",
    "    f'KID Score (Lower is Better) for {run_name}', 'KID', 'min', 'tab:orange'\n",
    ")\n",
    "\n",
    "# IS\n",
    "plot_scores_with_annotations(\n",
    "    epochs_evaluated_fid_kid_is, is_scores, best_scores['is'],\n",
    "    f'Inception Score (Higher is Better) for {run_name}', 'IS', 'max', 'tab:green'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05c4aa4-91e9-4cc4-88c7-9999b097c1c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
